---
title: "Основы программирования в R"
subtitle: "Предварительная обработка текста в R и облака слов"
author: "Алла Тамбовцева"
date: '20 декабря 2017 г '
output: html_document
---

### Предварительная обработка текста

В классических методах анализа текстов (имеются в виду "классическое" машинное обучение, не глубинное обучение, не нейронные сети) часто используется такой подход к тексту, который называется **мешок слов** (**bag of words**): при анализе порядок слов, грамматическая и синтаксическая структура текста не учитываются. 

Какие этапы обработки текста существуют? Другими словами, что нужно сделать с текстом, прежде, чем переходить к содержательно части анализа?

1. **Токенизация**: разбиение текста на предложения, а предложений -- на слова. Обычно по умолчанию разбиение на предложения происходит по знакам препинания, на слова -- по пробелам, поэтому в качестве токена (минимальной единицы текста) выбирается "слово" -- последовательность символов, отделенная от других последовательностей пробелом слева и справа. Это может иногда создавать трудности (подумайте о сочетаниях типа *г.Москва* или *опеч атка*).

2. **Стемминг**: сокращение слова до его основы. Нужен для того, чтобы разные формы слова воспринимались как одно и то же слово, а не как разные. Например, *депутату*, *депутата* -- это просто формы слова *депутат* [основа *депутат*], а *прочитал*, *прочитала*, *прочитаю* -- разные формы глагола *прочитать* [основа *прочита*]. 

3. **Лемматизация**: приведение слова к начальной форме (лемме -- тому виду, в котором оно встречается в словаре). Как и стемминг, нужна для того, чтобы распознавать разные формы слова как одно и то же слово. В отличие от стемминга, ловит более сложные случаи (например, стемминг не позволит распознать слово *хуже* как форму наречия *плохо*, а лемматизация позволит). Если не совсем понятна разница между стеммингом и лемматизацией, еще пример:

* слово *политологической* после стемминга: *политологическ*
* слово *политологической* после лемматизации: *политологический*

4. **Удаление стоп-слов**. Стоп-слова -- слова, которые часто встречаются в языке и которые не несут серьезной содержательной информации. Как правило, к стоп-словам относят предлоги, частицы, союзы и прочие несамостоятельные части речи, а также местоимения разных видов. Но иногда в список стоп-слов можно включить вполне самостоятельные слова. Например, стоп-словом при анализе запросов можно считать слово *ищу*, если почти каждый пользователь его использует (понятно, что дополнительной смысловой нагрузки это слово не несет -- раз все запросы про поиск чего-то).

### Облака слов

Облако слов (*word cloud*) -- средство визуализации встречаемости слов в тексте (текстах). Наверное, многие знакомы с облаками слов, но на всякий случай, вот [примеры](https://www.wordclouds.com/gallery/) облаков слов. Облака слов бывают очень разные и по форме, и по цвету, но главная идея состоит в следующем: чем больше размер слова в облаке, тем чаще оно встречается в тексте. 

Давайте построим первое облако слов. Для начала возьмем текст на английском -- "Рождественская песнь в прозе" Ч.Диккенса (*A Christmas Carol*). 

Загрузим текст (обычный txt-файл):

```{r}
text <- readLines("ccarol.txt") 
```

Посмотрим на первые 20 строк:

```{r}
head(text, 20)
```

Теперь загрузим библиотеку `tm` (от *text mining*) и выполним предварительную обработку текста. (Еще понадобиться загрузить библиотеку `SnowballC` -- для стемминга).

[Код далее частично основан на этих [материалах](http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know)].

```{r, warning=FALSE, message=FALSE}
# install.packages("tm")
# install.packages("SnowballC") 
library(tm)
```

Создадим корпус -- набор текстов.

```{r}
corp <- Corpus(VectorSource(text))
#inspect(corp)
```

Превратим все буквы в строчные:

```{r}
corp <- tm_map(corp, content_transformer(tolower))
```

Удалим цифры, пунктуацию и лишние пробелы:

```{r}
corp <- tm_map(corp, removeNumbers)
corp <- tm_map(corp, removePunctuation)
corp <- tm_map(corp, stripWhitespace)
```

Удалим стоп-слова для английского языка (а сначала на них посмотрим): 

```{r}
stopwords("english")
corp <- tm_map(corp, removeWords, stopwords("english"))
```

Добавим в список стоп-слов свои стоп-слова и удалим их:

```{r}
corp <- tm_map(corp, removeWords, c("scrooge", "said", "upon"))
```

А теперь выполним стемминг:

```{r}
corp <- tm_map(corp, stemDocument)
```

Посмотрим, как выглядит корпус сейчас:

```{r, eval = FALSE}
inspect(corp)
```

Создадим матрицу слово-документ (term-document matrix). В нашем случае это не так наглядно, потому что, строго говоря, документ у нас один -- один txt-файл:

```{r}
dtm <- TermDocumentMatrix(corp)
m <- as.matrix(dtm) # превратим в обычную матрицу
vec <- sort(rowSums(m), decreasing = TRUE) # превратим в вектор с частотами, отсортированный по убыванию
head(vec, 10)
data <- data.frame(word = names(vec), freq = vec) # превратим в базу данных
head(data, 10) 
```

Установим библиотеку `wordcloud`, а заодно загрузим уже знакомую библиотеку `RColorBrewer` -- для палитр цветов:

```{r}
# install.packages("wordcloud")
library(wordcloud)
library(RColorBrewer)
```

Теперь наконец-то построим облако слов:

```{r, warning=FALSE}
# для воспроизводимости - R будет располагать слова в случайном порядке
set.seed(1234) 

# min.freq - минимальная частота слова, которое отображается в облаке
# max.words - максимальное число слов в облаке
# colors - палитра цветов

pdf("wc.pdf")
wordcloud(words = data$word, freq = data$freq, min.freq = 20,
          max.words = 100, random.order = FALSE, 
          colors = brewer.pal(8, "Dark2"),
          rot.per = 0.9)
```

Наверное, в случае с облаком слов, стемминг немного портит картину: с одной стороны, он делает полезное дело, избавляя нас от разных форм одного и того же слова, с другой -- обрезанные слова в облаке выглядят неэтетично. Как это исправить? Использовать вместо стемминга лемматизацию! Тогда слова будут приводиться к начальной форме, и в облаке мы будем видеть не основу слова, а само слово.

Есть небольшая проблема: в R сложно найти какой-то легкий способ осуществить лемматизацию. Самый распространенный способ -- использовать оболочку для TreeTagger из библиотеки `koRpus`. Но для этого нужно установить не только саму библиотеку, но и [TreeTagger](http://www.cis.uni-muenchen.de/~schmid/tools/TreeTagger/). Давайте пока не будем обсуждать лемматизацию отдельно, подробности см. [здесь](http://www.bernhardlearns.com/2017/04/cleaning-words-with-r-stemming.html). 



